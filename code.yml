{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "514916f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datefinder\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pymysql\n",
    "from pandasql import sqldf\n",
    "from functools import reduce\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "import pytz\n",
    "import datetime as dt \n",
    "from stat import S_ISREG, ST_CTIME, ST_MODE\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "from google.cloud import bigquery\n",
    "import re\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pickle\n",
    "import os.path\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "import calendar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "416d9194",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\buywo\\\\Downloads\\\\Code\\\\bigquery-project-372513-f598e618189f.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m service_account_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbuywo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbigquery-project-372513-f598e618189f.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# create credentials using the service account key file\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m credentials \u001b[38;5;241m=\u001b[39m \u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCredentials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_service_account_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.googleapis.com/auth/cloud-platform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# specify the name of the BigQuery dataset and table\u001b[39;00m\n\u001b[0;32m     52\u001b[0m dataset_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msandbox\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\oauth2\\service_account.py:241\u001b[0m, in \u001b[0;36mCredentials.from_service_account_file\u001b[1;34m(cls, filename, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_service_account_file\u001b[39m(\u001b[38;5;28mcls\u001b[39m, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;124;03m\"\"\"Creates a Credentials instance from a service account json file.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m            credentials.\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m     info, signer \u001b[38;5;241m=\u001b[39m \u001b[43m_service_account_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_filename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequire\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclient_email\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_uri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_signer_and_info(signer, info, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\google\\auth\\_service_account_info.py:80\u001b[0m, in \u001b[0;36mfrom_filename\u001b[1;34m(filename, require, use_rsa_signer)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_filename\u001b[39m(filename, require\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, use_rsa_signer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;124;03m\"\"\"Reads a Google service account JSON file and returns its parsed info.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m            info and a signer instance.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[0;32m     81\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m data, from_dict(data, require\u001b[38;5;241m=\u001b[39mrequire, use_rsa_signer\u001b[38;5;241m=\u001b[39muse_rsa_signer)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\buywo\\\\Downloads\\\\Code\\\\bigquery-project-372513-f598e618189f.json'"
     ]
    }
   ],
   "source": [
    "tgt_cols = ['url',\n",
    "'pincode',\n",
    "'warning',\n",
    "'error',\n",
    "'title',\n",
    "'mrp_amt',\n",
    "'price_final',\n",
    "'imgages_cnt',\n",
    "'seller1',\n",
    "'seller2',\n",
    "'seller3',\n",
    "'delivery_time',\n",
    "'ratings',\n",
    "'no_of_ratings',\n",
    "'seller1_sellprice_amt',\n",
    "'seller2_sellprice_amt',\n",
    "'seller3_sellprice_amt',\n",
    "'seller1_tat_days',\n",
    "'seller2_tat_days',\n",
    "'seller3_tat_days',\n",
    "'item_weight',\n",
    "'item_dimensions',\n",
    "'bsr',\n",
    "'report_name',\n",
    "'platform',\n",
    "'brand_name',\n",
    "'load_dttm',\n",
    "'run_date',\n",
    "'delivered_by_seller1',\n",
    "'delivered_by_seller2',\n",
    "'delivered_by_seller3',\n",
    "'bsr1_rnk',\n",
    "'bsr1_category',\n",
    "'bsr2_rnk',\n",
    "'bsr2_category',\n",
    "'bsr3_rnk',\n",
    "'bsr3_category',\n",
    "'bsr4_rnk',\n",
    "'bsr4_category']\n",
    "# specify your Google Cloud Project ID\n",
    "project_id = \"bigquery-project-372513\"\n",
    "\n",
    "# specify the path to your service account key file\n",
    "service_account_file = r\"C:\\Users\\buywo\\Downloads\\Code\\bigquery-project-372513-f598e618189f.json\"\n",
    "\n",
    "# create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    service_account_file,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "# specify the name of the BigQuery dataset and table\n",
    "dataset_id = \"sandbox\"\n",
    "table_id = \"mpcrawlpdp_rpt\"\n",
    "\n",
    "def day_diff(start=None, end=None):\n",
    "    \"\"\"Get number of days between start and end columns in pandas dataframe\n",
    "\n",
    "    Args:\n",
    "        start (datetime64[ns]): Start Date. Default (None)\n",
    "        end (datetime64[ns]): Start Date. Default (None)\n",
    "\n",
    "    Returns:\n",
    "        int: Number of days (End - Start)\n",
    "    \n",
    "    Example:\n",
    "        df.apply(lambda rows: day_diff(rows['start_date'],rows['end_date']),axis=1)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return (end.date()-start.date()).days\n",
    "    except:\n",
    "        return None\n",
    "def get_days_to_deliver(run_dt,delivery_date_string):\n",
    "    string = str(delivery_date_string).lower()\n",
    "    run_dt=pd.to_datetime(run_dt,format='%d-%m-%Y')\n",
    "    \n",
    "    if ((not string) \n",
    "        or (string == 'nan')\n",
    "        or (\"Information\".lower() in string) \n",
    "        or (\"Loading\".lower() in string) \n",
    "        or (string.startswith(\"à\"))):\n",
    "        return None\n",
    "\n",
    "    elif \"Today\".lower() in string:\n",
    "        \n",
    "        return 0\n",
    "\n",
    "    elif \"Tomorrow\".lower() in string:\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    elif \"free delivery in\".lower() in string:\n",
    "        \n",
    "        return string.split(\",\")[0].split(\" \")[-2]\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            matches = list(datefinder.find_dates(string.split(\"-\")[0]))\n",
    "#             print(run_dt, matches, string)\n",
    "            if run_dt.month > matches[0].month:\n",
    "                delivery_date_year = run_dt.year+1\n",
    "            else:\n",
    "                delivery_date_year = run_dt.year\n",
    "\n",
    "            delivery_date_month = matches[0].month\n",
    "            delivery_date_day = matches[0].day\n",
    "    #         print(delivery_date_year)\n",
    "            delivery_date = datetime.datetime(year=delivery_date_year, month=delivery_date_month, day=delivery_date_day)\n",
    "    #         print(delivery_date)\n",
    "            return day_diff(start=run_dt,end=delivery_date)\n",
    "        except:\n",
    "            return None    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ac6dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define function to clean column names\n",
    "def clean_column_name(column):\n",
    "    column = column.lower().replace(\" (rs.)\",\"\").replace(\"(rs.)\",\"\").replace(\" (%)\",\"\").replace(\"(\",\"\").replace(\")\",\"\").strip().replace(\" \",\"_\").replace(\"/\",\"_\").replace(\"-\",\"_\").replace(\"=\",\"\").replace(\"+\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"&\",\"\").replace(\".\",\"\").replace(\"'\",\"\").replace(\"?\",\"\").replace(\"__\",\"_\").replace(\":\",\"\").replace(\"__\",\"_\")\n",
    "    return column\n",
    "def read_crawl_amazon(path, file):\n",
    "    df = pd.read_excel(path+\"\"\"\\\\\"\"\"+file,sheet_name='Amazon' ,engine='openpyxl').astype(str)\n",
    "    \n",
    "    df['report_name'] = file\n",
    "    df['platform'] = 'amazon'\n",
    "    df['brand_name'] = 'wow'\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df['load_dttm']=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for files in file:\n",
    "        run_date = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "        if 'CollectorM' in file:\n",
    "            run_date += ' 10:00'\n",
    "        elif 'CollectorA' in files:\n",
    "            run_date += ' 14:00'\n",
    "        elif 'Collector' in file:\n",
    "            run_date += ' 14:00'\n",
    "        df['run_date'] = pd.to_datetime(run_date, format=\"%d-%m-%Y %H:%M\")\n",
    "    #df['run_date'] = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    df = df.rename(columns = {\n",
    "        'input_url':'url',\n",
    "        'input_zip_code':'pincode',\n",
    "        'product_title':'title',\n",
    "        'price_mrp':'mrp_amt',\n",
    "        'no_of_product_images':'imgages_cnt',\n",
    "        'seller_1':'seller1',\n",
    "        'seller_2':'seller2',\n",
    "        'rating':'ratings',\n",
    "        'seller_3':'seller3',\n",
    "        'seller_1_price_final':'seller1_sellprice_amt',\n",
    "        'seller_2_price_final':'seller2_sellprice_amt',\n",
    "        'seller_3_price_final':'seller3_sellprice_amt',\n",
    "        'seller_1_delivery_time':'seller1_tat_days',\n",
    "        'seller_2_delivery_time':'seller2_tat_days',\n",
    "        'seller_3_delivery_time':'seller3_tat_days'})\n",
    "    \n",
    "    df['delivered_by_seller1'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller1_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    df['delivered_by_seller2'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller2_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    df['delivered_by_seller3'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller3_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    \n",
    "    if 'bsr' in df.columns.tolist():\n",
    "    # Process BSR column\n",
    "        for i in df.itertuples():\n",
    "            bsr = i.bsr\n",
    "            if \"#\" in str(bsr):    \n",
    "                bsr = bsr.strip().split(\"#\")\n",
    "                bsr_lst = [i[:i.find('(')].strip().replace(\".\", \"\").replace(\",\", \"\").split(\" in \")\n",
    "                           for i in bsr if len(i) > 0]\n",
    "                bsr_no = 0\n",
    "                for j in bsr_lst:\n",
    "                    if len(j)==2:\n",
    "                        bsr_no += 1\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_rnk\"] = j[0]\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_category\"] = j[1]\n",
    "    return df\n",
    "\n",
    "# get the list of files in the directory\n",
    "dir_path = \"C:\\\\Users\\\\buywo\\\\Downloads\\\\new_crawL_data\\\\amazon_crawl_data\"\n",
    "files = [f for f in os.listdir(dir_path) if (f.split(\".\")[1] == 'xlsx')]\n",
    "\n",
    "# check if the table exists in BigQuery\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "# df1 = pd.DataFrame()\n",
    "for file in files:\n",
    "    print(file, os.path.getmtime(os.path.join(dir_path, file)))\n",
    "    df = read_crawl_amazon(dir_path,file)\n",
    "    report_names=pd.DataFrame()\n",
    "#     if not table_exists:\n",
    "#         schema = []\n",
    "#         for column in df.columns:\n",
    "#             if df[column].dtype == \"O\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"STRING\"))\n",
    "#             elif df[column].dtype == \"float64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"FLOAT\"))\n",
    "#             elif df[column].dtype == \"int64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"INTEGER\"))\n",
    "#             elif df[column].dtype == \"datetime64[ns]\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"TIMESTAMP\"))\n",
    "#         table = bigquery.Table(table_ref, schema=schema)\n",
    "#         client.create_table(table)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        table_exists = True\n",
    "    except:\n",
    "        table_exists = False\n",
    "    if table_exists:   \n",
    "        query = f\"SELECT DISTINCT report_name FROM `{project_id}.{dataset_id}.{table_id}` where report_name = '{file}'\"\n",
    "        report_names = pandas_gbq.read_gbq(query, project_id=project_id, credentials=credentials)\n",
    "        print(report_names.shape, \"Data already exists\")\n",
    "    if report_names.shape[0]==0:\n",
    "        #check_and_correct_schema(project_id, dataset_id, table_id, df)\n",
    "        # insert the data from the Pandas dataframe into BigQuery\n",
    "        pandas_gbq.to_gbq(df[[i for i in df.columns.tolist() if i in tgt_cols]], f\"{dataset_id}.{table_id}\", project_id=project_id, if_exists=\"append\", credentials=credentials)\n",
    "        \n",
    "        print(f\"{file} uploaded to BigQuery.\") \n",
    "        \n",
    "\n",
    "\n",
    "#         df1 = pd.concat([df1,df],ignore_index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define function to clean column names\n",
    "def clean_column_name(column):\n",
    "    column = column.lower().replace(\" (rs.)\",\"\").replace(\"(rs.)\",\"\").replace(\" (%)\",\"\").replace(\"(\",\"\").replace(\")\",\"\").strip().replace(\" \",\"_\").replace(\"/\",\"_\").replace(\"-\",\"_\").replace(\"=\",\"\").replace(\"+\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"&\",\"\").replace(\".\",\"\").replace(\"'\",\"\").replace(\"?\",\"\").replace(\"__\",\"_\").replace(\":\",\"\").replace(\"__\",\"_\")\n",
    "    return column\n",
    "def read_crawl_flipkart(path, file):\n",
    "    df = pd.read_excel(path+\"\"\"\\\\\"\"\"+file, sheet_name='Filpkart' ,engine='openpyxl').astype(str)\n",
    "    \n",
    "    df['report_name'] = file\n",
    "    df['platform'] = 'flipkart'\n",
    "    df['brand_name'] = 'wow'\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df['load_dttm']=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for files in file:\n",
    "        run_date = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "        if 'CollectorM' in file:\n",
    "            run_date += ' 10:00'\n",
    "        elif 'CollectorA' in files:\n",
    "            run_date += ' 14:00'\n",
    "        elif 'Collector' in file:\n",
    "            run_date += ' 14:00'\n",
    "        df['run_date'] = pd.to_datetime(run_date, format=\"%d-%m-%Y %H:%M\")\n",
    "    #df['run_date'] = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    df = df.rename(columns = {\n",
    "        'input_url':'url',\n",
    "        'input_zip_code':'pincode',\n",
    "        'product_title':'title',\n",
    "        'price_mrp':'mrp_amt',\n",
    "        'no_of_product_images':'imgages_cnt',\n",
    "        'seller_1':'seller1',\n",
    "        'seller_2':'seller2',\n",
    "        'seller_3':'seller3',\n",
    "        'seller_1_price_final':'seller1_sellprice_amt',\n",
    "        'seller_2_price_final':'seller2_sellprice_amt',\n",
    "        'seller_3_price_final':'seller3_sellprice_amt',\n",
    "        'seller_1_delivery_time':'seller1_tat_days',\n",
    "        'seller_2_delivery_time':'seller2_tat_days',\n",
    "        'seller_3_delivery_time':'seller3_tat_days'})\n",
    "    \n",
    "    df['delivered_by_seller1'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller1_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    df['delivered_by_seller2'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller2_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    df['delivered_by_seller3'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller3_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    \n",
    "    if 'bsr' in df.columns.tolist():\n",
    "    # Process BSR column\n",
    "        for i in df.itertuples():\n",
    "            bsr = i.bsr\n",
    "            if \"#\" in str(bsr):    \n",
    "                bsr = bsr.strip().split(\"#\")\n",
    "                bsr_lst = [i[:i.find('(')].strip().replace(\".\", \"\").replace(\",\", \"\").split(\" in \")\n",
    "                           for i in bsr if len(i) > 0]\n",
    "                bsr_no = 0\n",
    "                for j in bsr_lst:\n",
    "                    if len(j)==2:\n",
    "                        bsr_no += 1\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_rnk\"] = j[0]\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_category\"] = j[1]\n",
    "    return df\n",
    "\n",
    "# get the list of files in the directory\n",
    "dir_path = \"C:\\\\Users\\\\buywo\\\\Downloads\\\\new_crawL_data\\\\flipkart_crawl_data\"\n",
    "files = [f for f in os.listdir(dir_path) if (f.split(\".\")[1] == 'xlsx')]\n",
    "\n",
    "# check if the table exists in BigQuery\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "# df1 = pd.DataFrame()\n",
    "for file in files:\n",
    "    print(file, os.path.getmtime(os.path.join(dir_path, file)))\n",
    "    df = read_crawl_flipkart(dir_path,file)\n",
    "    report_names=pd.DataFrame()\n",
    "#     if not table_exists:\n",
    "#         schema = []\n",
    "#         for column in df.columns:\n",
    "#             if df[column].dtype == \"O\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"STRING\"))\n",
    "#             elif df[column].dtype == \"float64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"FLOAT\"))\n",
    "#             elif df[column].dtype == \"int64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"INTEGER\"))\n",
    "#             elif df[column].dtype == \"datetime64[ns]\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"TIMESTAMP\"))\n",
    "#         table = bigquery.Table(table_ref, schema=schema)\n",
    "#         client.create_table(table)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        table_exists = True\n",
    "    except:\n",
    "        table_exists = False\n",
    "    if table_exists:   \n",
    "        query = f\"SELECT DISTINCT report_name FROM `{project_id}.{dataset_id}.{table_id}` where report_name = '{file}'\"\n",
    "        report_names = pandas_gbq.read_gbq(query, project_id=project_id, credentials=credentials)\n",
    "        print(report_names.shape, \"Data already exists\")\n",
    "    if report_names.shape[0]==0:\n",
    "        #check_and_correct_schema(project_id, dataset_id, table_id, df)\n",
    "        # insert the data from the Pandas dataframe into BigQuery\n",
    "        pandas_gbq.to_gbq(df[[i for i in df.columns.tolist() if i in tgt_cols]], f\"{dataset_id}.{table_id}\", project_id=project_id, if_exists=\"append\", credentials=credentials)\n",
    "        \n",
    "        print(f\"{file} uploaded to BigQuery.\") \n",
    "        \n",
    "\n",
    "\n",
    "#         df1 = pd.concat([df1,df],ignore_index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define function to clean column names\n",
    "def clean_column_name(column):\n",
    "    column = column.lower().replace(\" (rs.)\",\"\").replace(\"(rs.)\",\"\").replace(\" (%)\",\"\").replace(\"(\",\"\").replace(\")\",\"\").strip().replace(\" \",\"_\").replace(\"/\",\"_\").replace(\"-\",\"_\").replace(\"=\",\"\").replace(\"+\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"&\",\"\").replace(\".\",\"\").replace(\"'\",\"\").replace(\"?\",\"\").replace(\"__\",\"_\").replace(\":\",\"\").replace(\"__\",\"_\")\n",
    "    return column\n",
    "def read_crawl_myntra(path, file):\n",
    "    df = pd.read_excel(path+\"\"\"\\\\\"\"\"+file, sheet_name='Myntra' ,engine='openpyxl').astype(str)\n",
    "    \n",
    "    df['report_name'] = file\n",
    "    df['platform'] = 'myntra'\n",
    "    df['brand_name'] = 'wow'\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df['load_dttm']=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for files in file:\n",
    "        run_date = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "        if 'CollectorM' in file:\n",
    "            run_date += ' 10:00'\n",
    "        elif 'CollectorA' in files:\n",
    "            run_date += ' 14:00'\n",
    "        elif 'Collector' in file:\n",
    "            run_date += ' 14:00'\n",
    "        df['run_date'] = pd.to_datetime(run_date, format=\"%d-%m-%Y %H:%M\")\n",
    "    \n",
    "    #df['run_date'] = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    df = df.rename(columns = {\n",
    "        'input_url':'url',\n",
    "        'input_zip_code':'pincode',\n",
    "        'product_title':'title',\n",
    "        'price_mrp':'mrp_amt',\n",
    "        'no_of_product_images':'imgages_cnt',\n",
    "        'seller_1':'seller1',\n",
    "        'seller_2':'seller2',\n",
    "        'seller_3':'seller3',\n",
    "        'seller_1_price_final':'seller1_sellprice_amt',\n",
    "        'seller_2_price_final':'seller2_sellprice_amt',\n",
    "        'seller_3_price_final':'seller3_sellprice_amt',\n",
    "        'seller_1_delivery_time':'seller1_tat_days',\n",
    "        'seller_2_delivery_time':'seller2_tat_days',\n",
    "        'seller_3_delivery_time':'seller3_tat_days'})\n",
    "    \n",
    "    df['delivered_by_seller1'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller1_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    df['delivered_by_seller2'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller2_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    df['delivered_by_seller3'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller3_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    \n",
    "    if 'bsr' in df.columns.tolist():\n",
    "    # Process BSR column\n",
    "        for i in df.itertuples():\n",
    "            bsr = i.bsr\n",
    "            if \"#\" in str(bsr):    \n",
    "                bsr = bsr.strip().split(\"#\")\n",
    "                bsr_lst = [i[:i.find('(')].strip().replace(\".\", \"\").replace(\",\", \"\").split(\" in \")\n",
    "                           for i in bsr if len(i) > 0]\n",
    "                bsr_no = 0\n",
    "                for j in bsr_lst:\n",
    "                    if len(j)==2:\n",
    "                        bsr_no += 1\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_rnk\"] = j[0]\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_category\"] = j[1]\n",
    "    return df\n",
    "\n",
    "# get the list of files in the directory\n",
    "dir_path = \"C:\\\\Users\\\\buywo\\\\Downloads\\\\new_crawL_data\\\\myntra_crawl_dtata\"\n",
    "files = [f for f in os.listdir(dir_path) if (f.split(\".\")[1] == 'xlsx')]\n",
    "\n",
    "# check if the table exists in BigQuery\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "# df1 = pd.DataFrame()\n",
    "for file in files:\n",
    "    print(file, os.path.getmtime(os.path.join(dir_path, file)))\n",
    "    df = read_crawl_myntra(dir_path,file)\n",
    "    report_names=pd.DataFrame()\n",
    "#     if not table_exists:\n",
    "#         schema = []\n",
    "#         for column in df.columns:\n",
    "#             if df[column].dtype == \"O\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"STRING\"))\n",
    "#             elif df[column].dtype == \"float64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"FLOAT\"))\n",
    "#             elif df[column].dtype == \"int64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"INTEGER\"))\n",
    "#             elif df[column].dtype == \"datetime64[ns]\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"TIMESTAMP\"))\n",
    "#         table = bigquery.Table(table_ref, schema=schema)\n",
    "#         client.create_table(table)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        table_exists = True\n",
    "    except:\n",
    "        table_exists = False\n",
    "    if table_exists:   \n",
    "        query = f\"SELECT DISTINCT report_name FROM `{project_id}.{dataset_id}.{table_id}` where report_name = '{file}'\"\n",
    "        report_names = pandas_gbq.read_gbq(query, project_id=project_id, credentials=credentials)\n",
    "        print(report_names.shape, \"Data already exists\")\n",
    "    if report_names.shape[0]==0:\n",
    "        #check_and_correct_schema(project_id, dataset_id, table_id, df)\n",
    "        # insert the data from the Pandas dataframe into BigQuery\n",
    "        pandas_gbq.to_gbq(df[[i for i in df.columns.tolist() if i in tgt_cols]], f\"{dataset_id}.{table_id}\", project_id=project_id, if_exists=\"append\", credentials=credentials)\n",
    "        \n",
    "        print(f\"{file} uploaded to BigQuery.\") \n",
    "        \n",
    "\n",
    "\n",
    "#         df1 = pd.concat([df1,df],ignore_index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df87a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define function to clean column names\n",
    "def clean_column_name(column):\n",
    "    column = column.lower().replace(\" (rs.)\",\"\").replace(\"(rs.)\",\"\").replace(\" (%)\",\"\").replace(\"(\",\"\").replace(\")\",\"\").strip().replace(\" \",\"_\").replace(\"/\",\"_\").replace(\"-\",\"_\").replace(\"=\",\"\").replace(\"+\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"&\",\"\").replace(\".\",\"\").replace(\"'\",\"\").replace(\"?\",\"\").replace(\"__\",\"_\").replace(\":\",\"\").replace(\"__\",\"_\")\n",
    "    return column\n",
    "def read_crawl_nykaa(path, file):\n",
    "    df = pd.read_excel(path+\"\"\"\\\\\"\"\"+file, sheet_name='Nykaa' ,engine='openpyxl').astype(str)\n",
    "    \n",
    "    df['report_name'] = file\n",
    "    df['platform'] = 'nykaa'\n",
    "    df['brand_name'] = 'wow'\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df['load_dttm']=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for files in file:\n",
    "        run_date = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "        if 'CollectorM' in file:\n",
    "            run_date += ' 10:00'\n",
    "        elif 'CollectorA' in files:\n",
    "            run_date += ' 14:00'\n",
    "        elif 'Collector' in file:\n",
    "            run_date += ' 14:00'\n",
    "        df['run_date'] = pd.to_datetime(run_date, format=\"%d-%m-%Y %H:%M\")\n",
    "    #df['run_date'] = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    df = df.rename(columns = {\n",
    "        'input_url':'url',\n",
    "        'input_zip_code':'pincode',\n",
    "        'product_title':'title',\n",
    "        'price_mrp':'mrp_amt',\n",
    "        'no_of_product_images':'imgages_cnt',\n",
    "        'seller':'seller1',\n",
    "        'price_final':'seller1_sellprice_amt',\n",
    "        'delivery':'seller1_tat_days'})\n",
    "    \n",
    "    df['delivered_by_seller1'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller1_tat_days']), axis=1).fillna(0).astype(int)\n",
    "#     df['delivered_by_seller2'] = df.apply(\n",
    "#         lambda rows: get_days_to_deliver(rows['run_date'], rows['seller2_tat_days']), axis=1).fillna(0).astype(int)\n",
    "#     df['delivered_by_seller3'] = df.apply(\n",
    "#         lambda rows: get_days_to_deliver(rows['run_date'], rows['seller3_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    \n",
    "    if 'bsr' in df.columns.tolist():\n",
    "    # Process BSR column\n",
    "        for i in df.itertuples():\n",
    "            bsr = i.bsr\n",
    "            if \"#\" in str(bsr):    \n",
    "                bsr = bsr.strip().split(\"#\")\n",
    "                bsr_lst = [i[:i.find('(')].strip().replace(\".\", \"\").replace(\",\", \"\").split(\" in \")\n",
    "                           for i in bsr if len(i) > 0]\n",
    "                bsr_no = 0\n",
    "                for j in bsr_lst:\n",
    "                    if len(j)==2:\n",
    "                        bsr_no += 1\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_rnk\"] = j[0]\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_category\"] = j[1]\n",
    "    return df\n",
    "\n",
    "# get the list of files in the directory\n",
    "dir_path = \"C:\\\\Users\\\\buywo\\\\Downloads\\\\new_crawL_data\\\\nykaa_crawl_data\"\n",
    "files = [f for f in os.listdir(dir_path) if (f.split(\".\")[1] == 'xlsx')]\n",
    "\n",
    "# check if the table exists in BigQuery\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "# df1 = pd.DataFrame()\n",
    "for file in files:\n",
    "    print(file, os.path.getmtime(os.path.join(dir_path, file)))\n",
    "    df = read_crawl_nykaa(dir_path,file)\n",
    "    report_names=pd.DataFrame()\n",
    "#     if not table_exists:\n",
    "#         schema = []\n",
    "#         for column in df.columns:\n",
    "#             if df[column].dtype == \"O\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"STRING\"))\n",
    "#             elif df[column].dtype == \"float64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"FLOAT\"))\n",
    "#             elif df[column].dtype == \"int64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"INTEGER\"))\n",
    "#             elif df[column].dtype == \"datetime64[ns]\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"TIMESTAMP\"))\n",
    "#         table = bigquery.Table(table_ref, schema=schema)\n",
    "#         client.create_table(table)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        table_exists = True\n",
    "    except:\n",
    "        table_exists = False\n",
    "    if table_exists:   \n",
    "        query = f\"SELECT DISTINCT report_name FROM `{project_id}.{dataset_id}.{table_id}` where report_name = '{file}'\"\n",
    "        report_names = pandas_gbq.read_gbq(query, project_id=project_id, credentials=credentials)\n",
    "        print(report_names.shape, \"Data already exists\")\n",
    "    if report_names.shape[0]==0:\n",
    "        #check_and_correct_schema(project_id, dataset_id, table_id, df)\n",
    "        # insert the data from the Pandas dataframe into BigQuery\n",
    "        pandas_gbq.to_gbq(df[[i for i in df.columns.tolist() if i in tgt_cols]], f\"{dataset_id}.{table_id}\", project_id=project_id, if_exists=\"append\", credentials=credentials)\n",
    "        \n",
    "        print(f\"{file} uploaded to BigQuery.\") \n",
    "        \n",
    "\n",
    "\n",
    "#         df1 = pd.concat([df1,df],ignore_index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ba25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define function to clean column names\n",
    "def clean_column_name(column):\n",
    "    column = column.lower().replace(\" (rs.)\",\"\").replace(\"(rs.)\",\"\").replace(\" (%)\",\"\").replace(\"(\",\"\").replace(\")\",\"\").strip().replace(\" \",\"_\").replace(\"/\",\"_\").replace(\"-\",\"_\").replace(\"=\",\"\").replace(\"+\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"&\",\"\").replace(\".\",\"\").replace(\"'\",\"\").replace(\"?\",\"\").replace(\"__\",\"_\").replace(\":\",\"\").replace(\"__\",\"_\")\n",
    "    return column\n",
    "def read_crawl_purplle(path, file):\n",
    "    df = pd.read_excel(path+\"\"\"\\\\\"\"\"+file, sheet_name='Purplle' ,engine='openpyxl').astype(str)\n",
    "    \n",
    "    df['report_name'] = file\n",
    "    df['platform'] = 'purplle'\n",
    "    df['brand_name'] = 'wow'\n",
    "    df = df.replace(r'\\n',' ', regex=True)\n",
    "    df['load_dttm']=datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    df.columns = [clean_column_name(col) for col in df.columns]\n",
    "    for files in file:\n",
    "        run_date = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "        if 'CollectorM' in file:\n",
    "            run_date += ' 10:00'\n",
    "        elif 'CollectorA' in files:\n",
    "            run_date += ' 14:00'\n",
    "        elif 'Collector' in file:\n",
    "            run_date += ' 14:00'\n",
    "        df['run_date'] = pd.to_datetime(run_date, format=\"%d-%m-%Y %H:%M\")\n",
    "    #df['run_date'] = pd.to_datetime(file.split('.')[0].split(\" \")[-1], format=\"%d-%b-%y\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    df = df.rename(columns = {\n",
    "        'input_url':'url',\n",
    "        'input_zip_code':'pincode',\n",
    "        'product_title':'title',\n",
    "        'price_mrp':'mrp_amt',\n",
    "        'no_of_product_images':'imgages_cnt',\n",
    "        'seller':'seller1',\n",
    "        'price_final':'seller1_sellprice_amt',\n",
    "        'delivery':'seller1_tat_days'})\n",
    "    \n",
    "    df['delivered_by_seller1'] = df.apply(\n",
    "        lambda rows: get_days_to_deliver(rows['run_date'], rows['seller1_tat_days']), axis=1).fillna(0).astype(int)\n",
    "#     df['delivered_by_seller2'] = df.apply(\n",
    "#         lambda rows: get_days_to_deliver(rows['run_date'], rows['seller2_tat_days']), axis=1).fillna(0).astype(int)\n",
    "#     df['delivered_by_seller3'] = df.apply(\n",
    "#         lambda rows: get_days_to_deliver(rows['run_date'], rows['seller3_tat_days']), axis=1).fillna(0).astype(int)\n",
    "    \n",
    "    if 'bsr' in df.columns.tolist():\n",
    "    # Process BSR column\n",
    "        for i in df.itertuples():\n",
    "            bsr = i.bsr\n",
    "            if \"#\" in str(bsr):    \n",
    "                bsr = bsr.strip().split(\"#\")\n",
    "                bsr_lst = [i[:i.find('(')].strip().replace(\".\", \"\").replace(\",\", \"\").split(\" in \")\n",
    "                           for i in bsr if len(i) > 0]\n",
    "                bsr_no = 0\n",
    "                for j in bsr_lst:\n",
    "                    if len(j)==2:\n",
    "                        bsr_no += 1\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_rnk\"] = j[0]\n",
    "                        df.loc[i.Index, \"bsr\" + str(bsr_no) + \"_category\"] = j[1]\n",
    "    return df\n",
    "\n",
    "# get the list of files in the directory\n",
    "dir_path = \"C:\\\\Users\\\\buywo\\\\Downloads\\\\new_crawL_data\\\\purpple_crawl_data\"\n",
    "files = [f for f in os.listdir(dir_path) if (f.split(\".\")[1] == 'xlsx')]\n",
    "\n",
    "# check if the table exists in BigQuery\n",
    "client = bigquery.Client(project=project_id, credentials=credentials)\n",
    "table_ref = client.dataset(dataset_id).table(table_id)\n",
    "\n",
    "# df1 = pd.DataFrame()\n",
    "for file in files:\n",
    "    print(file, os.path.getmtime(os.path.join(dir_path, file)))\n",
    "    df = read_crawl_purplle(dir_path,file)\n",
    "    report_names=pd.DataFrame()\n",
    "#     if not table_exists:\n",
    "#         schema = []\n",
    "#         for column in df.columns:\n",
    "#             if df[column].dtype == \"O\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"STRING\"))\n",
    "#             elif df[column].dtype == \"float64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"FLOAT\"))\n",
    "#             elif df[column].dtype == \"int64\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"INTEGER\"))\n",
    "#             elif df[column].dtype == \"datetime64[ns]\":\n",
    "#                 schema.append(bigquery.SchemaField(column, \"TIMESTAMP\"))\n",
    "#         table = bigquery.Table(table_ref, schema=schema)\n",
    "#         client.create_table(table)\n",
    "\n",
    "    try:\n",
    "        client.get_table(table_ref)\n",
    "        table_exists = True\n",
    "    except:\n",
    "        table_exists = False\n",
    "    if table_exists:   \n",
    "        query = f\"SELECT DISTINCT report_name FROM `{project_id}.{dataset_id}.{table_id}` where report_name = '{file}'\"\n",
    "        report_names = pandas_gbq.read_gbq(query, project_id=project_id, credentials=credentials)\n",
    "        print(report_names.shape, \"Data already exists\")\n",
    "    if report_names.shape[0]==0:\n",
    "        #check_and_correct_schema(project_id, dataset_id, table_id, df)\n",
    "        # insert the data from the Pandas dataframe into BigQuery\n",
    "        pandas_gbq.to_gbq(df[[i for i in df.columns.tolist() if i in tgt_cols]], f\"{dataset_id}.{table_id}\", project_id=project_id, if_exists=\"append\", credentials=credentials)\n",
    "        \n",
    "        print(f\"{file} uploaded to BigQuery.\") \n",
    "        \n",
    "\n",
    "\n",
    "#         df1 = pd.concat([df1,df],ignore_index=False)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56ca91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from google.oauth2.service_account import Credentials\n",
    "from google.cloud import bigquery\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2cc5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to your JSON key file\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"C:\\Users\\buywo\\Downloads\\Code\\bigquery-project-372513-f598e618189f.json\"\n",
    "\n",
    "# Set up a client to connect to BigQuery\n",
    "client = bigquery.Client(project='bigquery-project-372513')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_google_creds():\n",
    "    scopes = [\n",
    "        'https://www.googleapis.com/auth/spreadsheets',\n",
    "        'https://www.googleapis.com/auth/drive'\n",
    "        ]\n",
    "    # Read the .json file and authenticate with the links\n",
    "    credentials = Credentials.from_service_account_file(\n",
    "            'bigquery-project-372513-f598e618189f.json',\n",
    "            scopes=scopes\n",
    "        )\n",
    "    \n",
    "    return credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad8529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_google_sheets(spreadsheet_id, tab_index, df):\n",
    "    # Link to authenticate\n",
    "    credentials = get_google_creds()\n",
    "    # Request authorization and open the selected spreadsheet\n",
    "    gc = gspread.authorize(credentials).open_by_key(spreadsheet_id)\n",
    "    # Select the worksheet\n",
    "    worksheet = gc.get_worksheet(tab_index)\n",
    "    # Clear all data in the worksheet\n",
    "    worksheet.clear()\n",
    "    # Convert the dataframe to a list of lists, including column headers\n",
    "    data = [df.columns.tolist()] + df.values.tolist()\n",
    "    # Insert the data into the worksheet\n",
    "    worksheet.insert_rows(data)\n",
    "    \n",
    "def get_data_google_sheets(sample_spreadsheet_id, tab_index):\n",
    "    # Link to authenticate\n",
    "    credentials = get_google_creds()\n",
    "    # Request authorization and open the selected spreadsheet\n",
    "    gc = gspread.authorize(credentials).open_by_key(sample_spreadsheet_id)\n",
    "    # Prompts for all spreadsheet values\n",
    "    values = gc.get_worksheet(tab_index).get_all_values()\n",
    "    # Turns the return into a dataframe\n",
    "    df = pd.DataFrame(values)\n",
    "    df.columns = df.iloc[0]\n",
    "    df.drop(df.index[0], inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5938fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = get_data_google_sheets(\"1jvfqlK8EXC_vkfD3Ax8vP4fIQlMtw9sNzOUcfqx3_nc\", 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643285f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_from_bq(query):\n",
    "    query_job = client.query(query)\n",
    "    results = query_job.result()\n",
    "    return results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db945507",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"DELETE\n",
    "FROM\n",
    "  `bigquery-project-372513.sandbox.price_input`\n",
    "WHERE\n",
    "  case when month like 'Jan%' then 1\n",
    "   when month like 'Feb%' then 2\n",
    "   when month like 'Mar%' then 3\n",
    "   when month like 'Apr%' then 4\n",
    "   when month like 'May%' then 5\n",
    "   when month like 'Jun%' then 6\n",
    "   when month like 'Jul%' then 7\n",
    "   when month like 'Aug%' then 8\n",
    "   when month like 'Sep%' then 9\n",
    "   when month like 'Oct%' then 10\n",
    "   when month like 'Nov%' then 11\n",
    "   when month like 'Dec%' then 12 end= EXTRACT(MONTH\n",
    "  FROM\n",
    "    current_date)\n",
    "  AND cast(right(month,4) as int) =  EXTRACT(YEAR FROM current_date)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acf4d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_input = fetch_data_from_bq(query)\n",
    "price_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d3470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "\n",
    "# specify your Google Cloud Project ID\n",
    "project_id = \"bigquery-project-372513\"\n",
    "\n",
    "# specify the path to your service account key file\n",
    "service_account_file = r\"C:\\Users\\buywo\\Downloads\\Code\\bigquery-project-372513-f598e618189f.json\"\n",
    "\n",
    "# create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    service_account_file,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# specify the name of the BigQuery dataset and table\n",
    "dataset_id = \"sandbox\"\n",
    "table_id = \"price_input\"\n",
    "\n",
    "# insert the data from the pandas dataframe into BigQuery\n",
    "pandas_gbq.to_gbq(df, f\"{dataset_id}.{table_id}\", project_id=project_id, if_exists=\"append\", credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984d7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cab2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = get_data_google_sheets(\"1jvfqlK8EXC_vkfD3Ax8vP4fIQlMtw9sNzOUcfqx3_nc\", 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acdb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea76f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df[(df['platform'].notna()) & (df['platform'] != '')]['platform']).reset_index(drop=True).merge(df['sku'], how='cross')\n",
    "\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_year = pd.Timestamp.now().year\n",
    "current_month = pd.Timestamp.now().month\n",
    "num_days = calendar.monthrange(current_year, current_month)[1]\n",
    "days = range(1, num_days+1)\n",
    "df_dates = pd.DataFrame({'day': days, 'month': [current_month]*num_days, 'year': [current_year]*num_days})\n",
    "\n",
    "df_dates['date'] = pd.to_datetime(df_dates[['year', 'month', 'day']])\n",
    "df_dates = df_dates.drop(['day', 'month', 'year'], axis=1)\n",
    "\n",
    "df_merged = df_dates.merge(df1, how='cross')\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2f5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data_google_sheets('1jvfqlK8EXC_vkfD3Ax8vP4fIQlMtw9sNzOUcfqx3_nc', 3, df_merged.reset_index(drop=True).astype('str'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16920db",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bq  = get_data_google_sheets(\"1jvfqlK8EXC_vkfD3Ax8vP4fIQlMtw9sNzOUcfqx3_nc\", 1 )\n",
    "table_bq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e16bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_from_bq(query):\n",
    "    query_job = client.query(query)\n",
    "    results = query_job.result()\n",
    "    return results.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5567a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"DELETE\n",
    "FROM\n",
    "  `bigquery-project-372513.sandbox.sale_type`\n",
    "WHERE\n",
    "  EXTRACT(MONTH\n",
    "  FROM\n",
    "    PARSE_DATE('%Y-%m-%d', date)) = EXTRACT(MONTH\n",
    "  FROM\n",
    "    current_date)\n",
    "  AND EXTRACT(YEAR\n",
    "  FROM\n",
    "    PARSE_DATE('%Y-%m-%d', date)) =  EXTRACT(YEAR FROM current_date)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_type = fetch_data_from_bq(query)\n",
    "sales_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b175c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "import pandas_gbq\n",
    "\n",
    "# specify your Google Cloud Project ID\n",
    "project_id = \"bigquery-project-372513\"\n",
    "\n",
    "# specify the path to your service account key file\n",
    "service_account_file = r\"C:\\Users\\buywo\\Downloads\\Code\\bigquery-project-372513-f598e618189f.json\"\n",
    "\n",
    "# create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    service_account_file,\n",
    "    scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# specify the name of the BigQuery dataset and table\n",
    "dataset_id = \"sandbox\"\n",
    "table_id = \"sale_type\"\n",
    "\n",
    "# insert the data from the pandas dataframe into BigQuery\n",
    "pandas_gbq.to_gbq(table_bq, f\"{dataset_id}.{table_id}\", project_id=project_id, if_exists=\"append\", credentials=credentials)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd6a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bq.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49740a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_bq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93315f14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
